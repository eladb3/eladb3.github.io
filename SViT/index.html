<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<!-- ======================================================================= -->
<!--<script src="http://www.google.com/jsapi" type="text/javascript"></script>-->
<!--<script type="text/javascript">google.load("jquery", "1.3.2");</script>-->
<style type="text/css">
    table {
        margin-bottom: 5px;
        margin-top: 5px;
    }
    body {
        font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight: 300;
        font-size: 18px;
        margin-left: auto;
        margin-right: auto;
        width: 70%;
    }

    h1 {
        font-weight: 300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px;
        -moz-border-radius: 10px;
        -webkit-border-radius: 10px;
    }

    a:link, a:visited {
        color: #1367a7;
        text-decoration: none;
    }

    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow: 0px 0px 1px 1px rgba(0, 0, 0, 0.35), /* The top layer shadow */ 5px 5px 0 0px #fff, /* The second layer */ 5px 5px 1px 1px rgba(0, 0, 0, 0.35), /* The second layer shadow */ 10px 10px 0 0px #fff, /* The third layer */ 10px 10px 1px 1px rgba(0, 0, 0, 0.35), /* The third layer shadow */ 15px 15px 0 0px #fff, /* The fourth layer */ 15px 15px 1px 1px rgba(0, 0, 0, 0.35), /* The fourth layer shadow */ 20px 20px 0 0px #fff, /* The fifth layer */ 20px 20px 1px 1px rgba(0, 0, 0, 0.35), /* The fifth layer shadow */ 25px 25px 0 0px #fff, /* The fifth layer */ 25px 25px 1px 1px rgba(0, 0, 0, 0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow: 0px 0px 1px 1px rgba(0, 0, 0, 0.35), /* The top layer shadow */ 5px 5px 0 0px #fff, /* The second layer */ 5px 5px 1px 1px rgba(0, 0, 0, 0.35), /* The second layer shadow */ 10px 10px 0 0px #fff, /* The third layer */ 10px 10px 1px 1px rgba(0, 0, 0, 0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr {
        border: 0;
        height: 1px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }


    #authors td {
        padding-bottom: 5px;
        padding-top: 30px;
    }
</style>


<head>
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-108048997-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-108048997-1');
  </script>

    <title>SViT</title>
    <meta property="og:title" content="Object-Region Video Transformers"/>
</head>

<body>
<br>
<center>
    <span style="font-size:36px">Bringing Image Scene Structure to Video via Frame-Clip Consistency of Object Tokens</span>
    <br>
    <br>
    <!-- <span style="font-size:22px">CVPR 2022</span> -->
    <br>
    <br>
<!--    <br>-->
    <table align=center>
        <tr>
            <span style="font-size:20px"><a href="https://www.linkedin.com/in/elad-ben-avraham-1a07a4169/">Elad Ben-Avraham</a><sup>1</sup></span> &nbsp;
            <span style="font-size:20px"><a href="https://roeiherz.github.io/">Roei Herzig</a><sup>1,3</sup></span> &nbsp;
            <span style="font-size:20px"><a href="https://karttikeya.github.io/">Karttikeya Mangalam</a><sup>2</sup></span> &nbsp;
        </tr><br>
        <tr>
            <span style="font-size:20px"><a href="http://www.amirbar.net/">Amir Bar</a><sup>1</sup></span> &nbsp;
            <span style="font-size:20px"><a href="https://anna-rohrbach.net/">Anna Rohrbach</a><sup>2</sup></span> &nbsp;
            <span style="font-size:20px"><a href="https://www.linkedin.com/in/leonid-karlinsky-5a80491">Leonid Karlinsky</a><sup>3</sup></span> &nbsp;
<!--        </tr><br>-->
<!--        <tr>-->
            <span style="font-size:20px"><a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a><sup>2</sup></span> &nbsp;
            <span style="font-size:20px"><a href="http://www.cs.tau.ac.il/~gamir/">Amir Globerson</a><sup>1</sup></span> &nbsp;
        </tr>
    </table>

<!--    <span style="font-size:18px"><sup>*</sup>Equally contributed</span>-->
<!--    <br><br>-->
    <br>
    <table align=center>
        <tr>
            <td align=center>
                <center>
                    <span style="font-size:20px">
                        <sup>1</sup>Tel-Aviv University &nbsp;<sup>2</sup>UC Berkeley &nbsp;<sup>3</sup>IBM Research
                    </span>
                </center>
            </td>
        </tr>
    </table>
    <br>


<!--    <span style="font-size:24px">Preprint. Under review.</span>-->

    <table align=center>
        <tr>
            <td align=center>
                <center>
                    <span style="font-size:20px"><a href="TODOARXIV"> [Paper]</a></span>
                </center>
            </td>
            <td align=center>
                <center>
                        <span style="font-size:20px"><a href='https://github.com/eladb3/SViT'> [GitHub]</a></span>
                </center>
            </td>
            <td align=center>
                <center>
                        <span style="font-size:20px"><a href='data/2022SViT.txt'> [Bibtex]</a></span>
                </center>
            </td>
        </tr>
    </table>
<br><br><br>
<!-- <center><img src="data/figures/teaser_prompting_resized.png" align="middle" width="400" height="500" ></center></center> -->
<center><img src="data/figures/teaser_prompting_resized.png" align="middle"></center></center>
<!-- <center><embed src="data/figures/teaser_prompting5.pdf" align="middle"></center></center> -->
<!-- <center><embed src="data/figures/teaser_prompting5.pdf" width="500px" height="500px"></center></center> -->
<!-- <iframe src="data/figures/teaser_prompting5.pdf"></iframe> -->
<br><br>
Our approach SViT brings structured scene representations from still images into video.
We use the <bf>HA</bf>nd-<bf>O</bf>bject <bf>G</bf>raph (HAOG) annotations of still images that are only available during training,
and videos, which may come from a different domain, with their downstream task annotations.
We design a shared video & image transformer that can handle both video and image inputs.
During training, given an image, the patch and object tokens are processed together,
and the object tokens learn to predict the HAOG,
whereas given a video input, the transformer predicts the video label (e.g. <i>Washing</i>) based on the patch tokens.
During inference, the learned object prompts, which have captured the structured scene information from images, are used to predict the video label.
<!-- Our ORViT model incorporates object information into video transformer layers.
The figure shows the standard (uniformly spaced) transformer patch-tokens in <span style="color:Blue;">blue</span>,
and object-regions corresponding to detections in <span style="color:Orange;">orange</span>.
In ORViT any temporal patch-token (e.g., the patch in <b>black</b> at time T)
attends to all patch tokens (<span style="color:Blue;">blue</span>) and region tokens (<span style="color:Orange;">orange</span>).
This allows the new patch representation to be informed by the objects.
Our method shows strong performance improvement on multiple video understanding tasks and datasets,
demonstrating the value of a model that incorporates object representations into a transformer architecture. -->


<br><br><br>
<hr>
<table align=center>
    <center><h1>Abstract</h1></center>
</table>

Recent action recognition models have achieved impressive results by integrating objects, their locations and interactions. 
However, obtaining dense structured annotations for each frame is tedious and time-consuming, making these methods expensive to train and less scalable. 
At the same time, if a small set of annotated images is available, either within or outside the domain of interest, 
how could we leverage these for a video downstream task? We propose a learning framework StructureViT (SViT for short), 
which demonstrates how utilizing the structure of a small number of images only available during training can improve a video model. 
SViT relies on two key insights. First, as both images and videos contain structured information, 
we enrich a transformer model with a set of <i>object tokens</i> that can be used across images and videos. 
Second, the scene representations of individual frames in video should ``align'' with those of still images. 
This is achieved via a <i>Frame-Clip Consistency</i> loss, which ensures the flow of structured information between images and videos. 
We explore a particular instantiation of scene structure, namely a <i>Hand-Object Graph</i>, consisting of hands and objects with their locations as nodes, 
and physical relations of contact/no-contact as edges. SViT shows strong performance improvements on multiple video understanding tasks and datasets.
<!-- %; and it wins first place in the Ego4D CVPR'22 Object State Localization challenge. -->


<br><br><br>
<hr>
<table align=center>
    <center><h1 id="motivation">Motivation</h1></center>
</table>
<p>
    Semantic understanding of videos is a key challenge for machine vision and artificial intelligence. 
    It is intuitive that video models should benefit from incorporating scene structure, 
    e.g., the objects that appear in a video, their attributes, and the way they interact. 
    In fact, several works have shown that such ``object-centric'' models perform well on tasks such as action recognition. 
    However, most of these models require ground-truth structured annotations of video during training. 
    This is clearly very costly, time-consuming, and not scalable.
    <br>
    This begs the question, <i>could we still benefit from scene structure in a less costly way? </i>
    A possible direction is to explore methods that would only require sparsely annotated frames within a downstream video domain. 
    Moreover, when it comes to structured annotations of static images, 
    there are numerous datasets with annotations such as boxes, visual relationships, attributes, and segmentations. 
    Since we can not always expect to have annotated images that perfect align with our downstream video task, 
    how can these resources be used to build better video models? 
    In this work, we propose such an approach, 
    which is particularly well suited for transformer architectures and offers an effective mechanism to leverage image structure to improve video transformers.
</p>
<!-- <center><img src="data/videos/motivation.gif" align="middle" width="300"></center> -->


<!--        Model Section-->
<table align=center>
    <center><h1 id="block">SViT Approach</h1></center>
</table>
<p>
    Our approach SViT learns a structured shared representation that arises from the interaction between the two modalities of images and videos. 
    We consider the setting where the main goal is to learn video understanding downstream tasks while leveraging structured image data. 
    In this paper, we focus on the following video tasks: action recognition and object state classification & localization. 
    In training, we assume that we have access to task-specific video labels and structured scene annotations for images, <i>Hand-Object Graph</i>s.
    Based on the structured representations obtained by using the <i>object tokens</i> and regularized by the <i>Frame-Clip Consistency loss</i>.
    the inference is performed only for the downstream video task without explicitly using the structured image annotations. 
</p>

<table align=center>
    <center><h1 id="block">Hand-Object Graph</h1></center>
</table>
<p>
    As mentioned above, our motivation is to bring scene structure from still images into video. 
    In order to achieve this, 
    one component of this work is the construction of a semantic representation of the interactions between the hands and objects in the scene. 
    Specifically, we propose to use a graph-structure we <i>Hand-Object Graph</i> (HAOG). 
    The nodes of the HAOG represent two hands and two objects with their locations, whereas the edges correspond to physical properties such as contact/no contact. 
</p>

<center><img src="data/figures/haog.png" width="40%" height="40%" align="middle"></center>
<!-- <center><img src="data/figures/haog.png" align="middle"></center> -->
<hr><br>
<table align=center>
    <center><h1 id="model">SViT</h1></center>
</table>
<p>
Our shared video & image transformer model processes two different types of tokens: 
standard patch tokens from the images and videos (<span style="color:rgb(142.8, 170.85, 219.3)"><b>blue</b></span>) and the object prompts (<span style="color:rgb(196.35, 137.7, 219.3)"><b>purple</b></span>), 
that are transformed into object tokens (<span style="color:rgb(112.2, 48.45, 160.65)"><b>purple</b></span>) in the output. 
During training, the object tokens (<span style="color:rgb(112.2, 48.45, 160.65)"><b>purple</b></span>) are trained to predict the HAOG for still images. 
For video frames that have no HAOG annotation, we use our ``Frame-Clip'' loss to ensure consistency between the ``frame object tokens'' 
(resulting from processing the frames separately) and the ``clip object tokens'' (resulting from processing the frame as part of the video). 
Last, the final video downstream task prediction results from applying a video downstream task head on the average of the patch tokens in the transformer output 
(after they have interacted with the clip object tokens (<span style="color:rgb(112.2, 48.45, 160.65)"><b>purple</b></span>).
</p>
<br><br><br>
<!-- <center><img src="data/figures/model figure.png" align="middle" width="400" height="500"></center> -->
<center><img src="data/figures/model figure.png" align="middle"></center>
<br><br>
<hr>
<br>

<table align=center width=850px>
    <center><h1>Visualizations</h1></center>
</table>
<table>
    <tr>
        <center>
            <img src="data/figures/vis.png"/></center>
        <p> <b>Hand-Object Graphs</b> detected by SViT.</p>
    </tr>
</table>
<br><br>

<br>

<br><br>
<hr>
<br>

<table align=center>
    <center><h1>Paper</h1></center>
    <tr>
        <td><a href="TODOARXIV.pdf"><img class="layered-paper-big" style="height:175px; width: 150px; margin-bottom: 50px"
                                                                src="data/figures/paper.png"/></a></td>
        <td><a href="TODOARXIV.pdf"></a></td>
        <td>
                <span style="font-size:14pt">  Elad Ben-Avraham, Roei Herzig, Karttikeya Mangalam, Amir Bar, Leonid Karlinsky, Anna Rohrbach, Trevor Darrell, Amir Globerson<br>
              <i>Bringing Image Scene Structure to Video via Frame-Clip Consistency of Object Tokens</i><br>
            <!-- Arxiv<br> -->
            Hosted on <a href="TODOARXIV">arXiv</a>
                </span>
        </td>
        <span style="font-size:4pt"><a href="TODOARXIV.pdf"><br></a></span>
    </tr>
</table>
<hr>
<table align=center style="margin-bottom: 50px">
    <tr>
        <td>
            <center><h1>Related Works</h1></center>
            Our work builds and borrows code from <a href="https://github.com/facebookresearch/MViT">MViT</a>. If you found our work helpful, consider citing MViT as well.
        </td>
    </tr>
</table>

<hr>
<table align=center style="margin-bottom: 50px">
    <tr>
        <td>
            <left>
                <center><h1>Acknowledgements</h1></center>
            <!-- We would like to thank Tete Xiao, Medhini Narasimhan, Rodolfo (Rudy) Corona, and Colorado Reed for helpful feedback and discussions. -->
                This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant ERC HOLI 819080).
                Prof. Darrellâ€™s group was supported in part by DoD including DARPA's XAI, and LwLL programs,  as well as BAIR's industrial alliance programs.
            </left>
        </td>
    </tr>
</table>


</body>
</html>
